# ProxySQL C19 Patch: Multi-master read/write consistency enforcing in MySQL asynchronous clusters 
>NOTE: This patch require [MySQL >= 5.7.6 with --session-track-gtids=OWN_GTID](https://dev.mysql.com/doc/refman//8.0/en/server-system-variables.html#sysvar_session_track_gtids) and at least one memcached protocol capable server, e.g. [memcached](https://memcached.org/), [couchbase](https://www.couchbase.com/) or also [mysql with the memcached plugin](https://dev.mysql.com/doc/refman/8.0/en/innodb-memcached-setup.html).

>BEWARE: Right now this is still only a working POC, not more!

The Idea for this patch was born in 2019 when we start thinking a convenient method to extend, to laguages other than PHP, the MySQL Consistency enfocement policy implemented in our [mymysqlnd_ms](https://github.com/sergiotabanelli/mysqlnd_ms/) fork, hence, from `Consistency` and `2019`, the name `C19`. Furthermore, during Covid 19 lock down and forced quarantine this first POC has been implemented, hence, from `Covid 19`, the name `C19`. Below a short rationale and getting started.

Different types of MySQL cluster solutions offer different service and data consistency levels to their users. Any asynchronous MySQL replication cluster offers eventual consistency by default. A read executed on an asynchronous slave may return current, stale or no data at all, depending on whether the slave has replayed all changesets from master or not.
Applications using a MySQL replication cluster need to be designed to work correctly with eventual consistent data. In most cases, however, stale data is not acceptable. In those cases only certain slaves or even only master are allowed to achieve the required quality of service from the cluster.

New MySQL functionalities available in more recent versions, like [multi source replication](https://dev.mysql.com/doc/refman/5.7/en/replication-multi-source.html) or [group replication](https://dev.mysql.com/doc/refman/5.7/en/group-replication.html), allow multi-master clusters and need application strategies to avoid write conflicts and enforce write consistency for distinct write context partitions.

The excellent [ProxySQL](https://proxysql.com) already has a [Consistent read feature](https://proxysql.com/blog/proxysql-gtid-causal-reads/) but it does not cross client connection boundaries, that is: the consistency enforcement is limited to the current connection, but e.g. async web applications, where reads and writes are normaly on distinct http requests and, therefore, on distinct DB connectios, need a more complex read consistency enforcement policy. And also, `ProxySQL` does not have features for write conflicts management in multi-master asyncronous cluster scenarios. 
The `ProxySQL C19` patch adds theese features to standard `ProxySQL` and can therefore transparently choose MySQL replication nodes according to the read and write requested consistency, and this also on distinct MySQL connections and also on connections spread across multiple `ProxySQL` instances. 

To share consitency enforcement context info, the C19 patch, use memcached. Mecached can be considered not a valid choice due to its non persistent character, indeed, if the C19 patch loose connection to the memcached servers or consistency context stored on memcached keys become unavailable, no consistency will be enforced. To partially mitigate this issue and justify the choice we could consider that:
* Memcached is extremely fast, realy easy to setup and wildly known and used
* There are alternatives that support memcached comunication protocol with various degree of persitency functionalities
* For consistent reads, sporadic enforcement failures due to memcached unavailability, can be considered acceptable
* For consistent writes in multi master InnoDB clusters (group replication), enforcement failures, will not lead to replication breaks but only to a transaction rollback
* For consistent writes in multi master multi source replication cluster, enforcement failures, can lead to replication breaks, for those cases a more robust memcached protocol capable server with persistency and HA functionalities like [couchbase](https://www.couchbase.com/) is suggested
* For future releases something like a fallback strategy could be implemented
* For future releases memcached can also be used as shared query cache, this also considering that read consistency can be considered a perfect cache invalidation strategy, that is: every time a write occurs for that partition context, all cached queries belonging to that context could be invalidated
* For future releases support for redis cache could be easily added

## Consistency context partitions
Context partitions are sets of queries made by groups of clients (from here on 'participants') which need to share with each others the same configured isolated consistency context. With read consistency, a consistency context participant will read all writes made by other participants, itself included. With write consistency, in multi-master clusters, writes from all consistency context participants will always do not conflicts each others. Context partitions size can range from single query sent by a single client (eventual consistency) to global unique context partition which include all queries sent by all clients. Eventual consistency can indeed be considered as the smallest context partition, where every single query from every single client is a context partition. In a-syncronous or semi-syncronous clusters, smaller context partitions means better load distribution and performance. In `ProxySQL C19` patch context partitions are established through the use of placeholders. Placeholders are reserved tokens used in configuration values of the `memcached_hostgroups` `ProxySQL C19` table. The placeholder token will be expanded to the corresponding value at connection init, allowing consistency context establishment on a connection attribute basis (for a complete list see below). 

## Read consistency
A read context partition is a set of application reads made by a context participant that must always at least run against previous writes made by all other context participants.  
Starting from MySQL 5.7.6 the MySQL server features the [session-track-gtids](https://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_session_track_gtids) system variable, which, if set, will allow a client to be aware of the unique Global Transaction Identifier ([GTID](https://dev.mysql.com/doc/refman/5.7/en/replication-gtids.html)) assigned by MySQL to an executed transaction.

C19 read consistency has following rules: 
* Reads belonging to a context partition can safely run only on cluster nodes that have already replicated all previous same context partition writes. 
* Reads belonging to a context partition can safely run on cluster nodes that still have not replicated writes from all other contexts.

For read consistency the most common scenarios is context partitioning on HTTP user session id. With [mymysqlnd_ms](https://github.com/sergiotabanelli/mysqlnd_ms/) plugin, this can be easily achieved accessing the PHP internal session id, this because the plugin is an extension of the PHP language, but for ProxySQL there is no means to directly access an http application session id. The C19 patch use a simple hack to workaround this limitation at the cost of a small and simple web application change, the hack is that every MySQL user that connect to ProxySQL C19 can have a trailing session id identifying the session id and therfore the needed read consistency context, that is: suppose that the mysql user used by your web application is `SQLmyapp`, than you can append the session id to the MySQL user separated by `#`, e.g. in PHP

```
$sqluser = 'SQLmyapp' . '#' . session_id();
```
The C19 patch will then strip the session id part from the connected MySQL user and use it as read context partition identifier. 

Another method is to supply the session id in the query as a comment, e.g. 
```
$query = '/* c19_key=' . session_id() . '*/' . 'SELECT * FROM myrealquery';
```
This allow application users to always read them writes also if made in different connections and also if distributed on different application servers. Especially in async ajax scenarios, where reads and writes are often made on distinct http requests, user session partitioning is of great value and allow transparent migration to MySQL asyncronous clusters in almost all use cases with no or at most extremely small effort and application changes.   

## Write consistency
New MySQL functionalities like [multi source replication](https://dev.mysql.com/doc/refman/8.0/en/replication-multi-source.html) or [group replication](https://dev.mysql.com/doc/refman/8.0/en/group-replication.html) allow multi-master clusters and need application strategies to avoid write conflicts and enforce write consistency for distinct write context partitions. A write context partition is a set of application writes that, if run on distinct masters, can potentially conflict each others but that do not conflict with write sets from all other defined partitions. 

It is widely known that adding masters to MySQL clusters does not scale out and does not increase write performance, that is because all masters replicate the same amount of data, so write load will be repeated on every master. However, given that other masters do not have to do the same amount of processing that the original master had to do when it originally executed the transaction, they apply the changes faster, transactions are replicated in a format that is used to apply row transformations only, without having to re-execute transactions again. There are also much more to take into account for clusters configurations, in practice distinct write queries sent to distinct masters will almost always have better total throughput then the same group of queries sent to a single master (as an example see [an overview of the Group Replication performance](https://mysqlhighavailability.com/an-overview-of-the-group-replication-performance/) multi-master peak with flow-control disabled). So the major obstacles to achieve a certain degree of writes scale-out are write conflicts and replication lag. The idea behind `mymysqlnd_ms` write consistency implementation is to move replication lag and write conflicts management to the ProxySQL balancer, that can be considered a far more easier scale-out resource. To summarize the C19 patch write consistency implementation tries to put loads on easier scalable front ends with the objective to enhance response time on much harder scalable back ends.

For write consistency, scenarios strictly depend from your application requirements and can range from write context partitioning on MySQL user, the most common, to context partitioning on a user session basis as for the above explained read consistency.

>BEWARE: distinct write sets partitions must not intersect each others. e.g. if a write set include all writes to table A, no other write set partition should include writes to table A.

Server side write consistency has following rules: 
* Writes belonging to distinct context partitions can safely run concurrently on distinct MySQL masters without any data conflicts and replication issues.
* Writes belonging to the same context partition can safely run concurrently only on the same master. 
* Writes belonging to the same context partition can safely run **NON** concurrently (there are no still pending same context writes) on any masters that has already replicated all previous same context writes.

## A quick look

Clone the repository, go to the cloned directory, if you want, take a quick look at `my`, `myt`, `mytt`, `myttt`, `docker-compose.gr.yml` and `proxysql.c19.sql`

* the `my` script simply invoke mysql client with query passed as first parameter, user session id as second parameter and ProxySQL port prefixed with third parameter
* the `myt` run for first parameter times a group of query of 1 insert, one select of the previous insert, 2 updates that if executed in backgroung with another `myt` execution will conflicts each other. For every single query it invokes the `my` script
* the `mytt` runs second parameter background instances of the `myt` script each one against a distinct user session id
* the `myttt` runs third parameter background instances of the `mytt` script each one against a distinct proxysql instance 
* `docker-compose.gr.yml` runs 3 MySQL group replication nodes each one with ProxySQL binlog reader installed, 1 memcached instance and 2 ProxySQL with C19 patch instances 
* `proxysql.c19.sql` is the sql script used to initialize ProxySQL instance acording to the `docker-compose.gr.yml` running context

run the docker-compose.gr.yml, wait some time until all the containers has finish the entrypoints startup, and then run:

```
mysql -u radmin -pradmin -h 127.0.0.1 -P16032 <proxysql.c19.sql
mysql -u radmin -pradmin -h 127.0.0.1 -P26032 <proxysql.c19.sql
```

And then run:

```
myttt 50 5 2
```
you can change the first and second parameter but not third because `docker-compose.gr.yml` has only 2 instances of ProxySQL C19 patch

## Getting started

Right now, to build the C19 patch, You must clone the repository and run the ProxySQL build steps, all PRoxySQL standard Makefile targets should work, distribution package build targets included

Go to the directory where you cloned the repo (or unpacked the tarball) and run:

```
make
sudo make install
```
the patch add a new table named `memcached_hostgroups` to main ProxySQL schema:

```
```